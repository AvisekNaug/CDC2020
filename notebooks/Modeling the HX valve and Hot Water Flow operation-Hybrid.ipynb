{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%writefile param_sets.json\n",
    "\n",
    "seed_value = 123  # seed for the experiment\n",
    "Trial = 5  # number of the experiment\n",
    "\n",
    "period = 6  # the period to sample the data at. 1 period= 5 minutes\n",
    "\n",
    "inputcols = ['sat-oat', 'wbt', 'orh']  # input predictors\n",
    "x_loc = [0]  # input vars we want to plot in detailed plot\n",
    "outputcols = ['hx_vlv1*hw_sf']  # output targets\n",
    "input_timesteps = 1  # number of timesteps for the input sequence\n",
    "output_timesteps = 1  # number of timesteps for the output sequence\n",
    "\n",
    "# Smoothing\n",
    "smooth_data = False  # whetter to smooth the data or not\n",
    "order = 5  # order of the filer\n",
    "T = 300  # sampling_period in seconds\n",
    "fs = 1 / 300  # sample rate, Hz\n",
    "cutoff = 0.0001  # desired cutoff frequency of the filter, Hz\n",
    "\n",
    "# adjust out of phase data\n",
    "adjust_lag = False  # whether to adjust the lag for certain columns\n",
    "lag_columns = outputcols  # choose columns to adjust lag\n",
    "data_lag = -1  # lag by how many periods: negative means shift column upwards\n",
    "\n",
    "# aggregate data based on period\n",
    "aggregate_data = True  # aggregate data or not\n",
    "rolling_sum_target = []  # create sum aggregate for these columns\n",
    "rolling_mean_target = ['wbt', 'sat-oat','hx_vlv1', 'orh']  # create mean aggregate for these columns\n",
    "\n",
    "# create temporal batches of data: df2dflist\n",
    "days, hours = 7, 0\n",
    "\n",
    "# Custom way to create Training Data\n",
    "startweek = 15  # start week; indicates how large training set is\n",
    "data_weeks = 30  # Create a large initial block startweek-data_weeks weeks of training and testing data\n",
    "threshold = 0.01  # Values below which we consider as 0 output in hxvalves in \"feature_range\" attribute scale\n",
    "feature_range = (0, 1)  # Scaling range\n",
    "create_lag = 0  # Create further lags in the output\n",
    "scaling = True  # Scale the input and output features\n",
    "reshaping = True  # reshape data according to (batch_size, time_steps, features)\n",
    "\n",
    "# model configuration\n",
    "modelconfig = {\n",
    "    'lstm_hidden_units': 16,\n",
    "    'lstm_no_layers': 1,\n",
    "    'dense_hidden_units': 16,\n",
    "    'dense_no_layers': 4,\n",
    "    'retrain_from_layers': 0,\n",
    "    'stateful': False,\n",
    "    'testmodel_stateful':False,\n",
    "    'train_batchsize':32,\n",
    "    'test_batchsize':1, # we are doing online prediction\n",
    "    'train_epochs': 5000,\n",
    "}\n",
    "\n",
    "# wheter doing adaptive for fixed learning\n",
    "adaptive_control = True  # whether we relearn or keep it fixed\n",
    "path = '../results/' + outputcols[0] + '_model{}/'.format(\n",
    "    Trial) + 'adaptive/' * adaptive_control + 'fixed/' * (1 - adaptive_control)\n",
    "#!rm -rf ../results/lstm_hwe_trial8/adaptive\n",
    "\n",
    "#model design considerations\n",
    "modeldesigndone = False  # whether model will be reinitialized\n",
    "initial_epoch = 0  # the start epoch number for the training\n",
    "\n",
    "# These are automatically superseded and ignored if adaptive_control is set to False\n",
    "retain_prev_model = True  # retain weights of model from previous training\n",
    "freeze_model = False  # freeze weights of certain layers\n",
    "reinitialize = False  # reinitialize the weights of certain layers\n",
    "model_saved = False  # whether model has been saved once\n",
    "test_model_created = False  # create an idectical model for online predicton\n",
    "\n",
    "# data used for learning the model\n",
    "datapath = '../data/processed/buildingdata.pkl'\n",
    "\n",
    "# additional info\n",
    "addl = {\n",
    "    'metainfo': 'create a diff of sat and oat for hot water energy prediction as it is useful. See 1.0.8',\n",
    "    'names_abreviation': {\n",
    "        'oat':'Outside Air Temperature',\n",
    "        'orh':'Outside Air Relative Humidity',\n",
    "        'sat-oat' : 'Difference of Supply Air and Outside Air Temps ',\n",
    "        'ghi': 'Global Solar Irradiance',\n",
    "        'hw_sf':'Hot Water System Flow Rate',\n",
    "        'hx_vlv1':'Hot Water Valve %',\n",
    "        'hw_st':'Hot Water Supply Temperature',\n",
    "        'hx_vlv1*hw_sf': 'Hot Water Flow Valve Opening',\n",
    "        'wbt': 'Wet Builb Temperature',\n",
    "    }\n",
    "}\n",
    "x_lab = [addl['names_abreviation'][inputcols[i]] for i in x_loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed in numpy, Keras and TF for reproducability and Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Enable '0' or disable '-1' GPU use\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# including the project directory to the notebook level\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import parse\n",
    "import warnings\n",
    "from matplotlib.dates import date2num\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    import tensorflow as tf\n",
    "    #tf.random.set_seed(seed_value)\n",
    "    # for later versions: \n",
    "    tf.compat.v1.set_random_seed(seed_value)\n",
    "    config = tf.ConfigProto(log_device_placement=False)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    \n",
    "    from keras import backend as K\n",
    "    from nn_source import models as mp\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "from dataprocess import dataprocessor as dp\n",
    "from dataprocess import plotutils as pu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder to save models and tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the results directory\n",
    "try:\n",
    "    os.makedirs(path)\n",
    "except FileExistsError:\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        try:\n",
    "            shutil.rmtree(path + f)\n",
    "        except NotADirectoryError:\n",
    "            os.remove(path + f)\n",
    "        \n",
    "os.mkdir(path + 'loginfo')\n",
    "os.mkdir(path + 'normalplots')\n",
    "os.mkdir(path + 'detailedplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the experiment parameters and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save the values\n",
    "paramsdict = {\n",
    "    \n",
    "    'seed_value' : seed_value,\n",
    "    \n",
    "    'period':period,\n",
    "    \n",
    "    'inputcols':inputcols,\n",
    "    'x_loc': x_loc,\n",
    "    'outputcols':outputcols,\n",
    "    'input_timesteps':input_timesteps,\n",
    "    'output_timesteps':output_timesteps,\n",
    "    \n",
    "    'smooth_data': smooth_data,\n",
    "    'order' : 5,\n",
    "    'T' : T,\n",
    "    'fs' : fs,\n",
    "    'cutoff' : cutoff,\n",
    "    \n",
    "    'adjust_lag' : adjust_lag,\n",
    "    'lag_columns' : lag_columns,\n",
    "    'data_lag' : data_lag,\n",
    "    \n",
    "    'aggregate_data' : aggregate_data,\n",
    "    'rolling_sum_target' : rolling_sum_target,\n",
    "    'rolling_mean_target' : rolling_mean_target,\n",
    "    \n",
    "    'days':days,\n",
    "    'hours':hours,\n",
    "    \n",
    "    'startweek': startweek,\n",
    "    'data_weeks' : data_weeks,\n",
    "    'threshold': threshold,\n",
    "    'create_lag' : create_lag,\n",
    "    'scaling' : scaling,\n",
    "    'feature_range' : feature_range,\n",
    "    'reshaping' : reshaping,\n",
    "    \n",
    "    'modelconfig' : modelconfig,\n",
    "    \n",
    "    'adaptive_control':adaptive_control,\n",
    "    'path':path,\n",
    "    \n",
    "    'modeldesigndone' : modeldesigndone,\n",
    "    'initial_epoch' : initial_epoch,\n",
    "    \n",
    "    'retain_prev_model' : retain_prev_model,\n",
    "    'freeze_model' : freeze_model,\n",
    "    'reinitialize' : reinitialize,\n",
    "    'model_saved' : model_saved,\n",
    "    'test_model_created': test_model_created,\n",
    "    \n",
    "    'datapath' : datapath,\n",
    "    \n",
    "    'addl' : addl,\n",
    "}\n",
    "    \n",
    "# with open(path+'params.json', 'r') as fp:\n",
    "#     param2dict = json.load(fp)\n",
    "\n",
    "with open(path+'params.json', 'w') as fp:\n",
    "    json.dump(paramsdict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the pickled file for ahu data\n",
    "df1data = dp.readfile(datapath)\n",
    "\n",
    "# return pickled df\n",
    "df1 = df1data.return_df(processmethods=['file2df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# read the pickled file for ahu data\n",
    "df2data = dp.readfile('../data/processed/interpolated/wetbulbtemp.pkl')\n",
    "\n",
    "# return pickled df\n",
    "df2 = df2data.return_df(processmethods=['file2df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df = dp.merge_df_columns([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create additional Data columns as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sat-oat']= df['sat']-df['oat']\n",
    "df['hx_vlv1*hw_sf']=df['hx_vlv1']*df['hw_sf']/50\n",
    "# show data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if smooth_data:\n",
    "    df = dp.dfsmoothing(df=df,\n",
    "                        column_names=list(df.columns),\n",
    "                        order=order,\n",
    "                        Wn=cutoff,\n",
    "                        T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['hw_sf'][df['hw_sf']<=0]=0\n",
    "df['hx_vlv1'][df['hx_vlv1']<=0]=0\n",
    "# df['sat-oat'][df['sat-oat']<=0]= 0\n",
    "df['hx_vlv1*hw_sf'][df['hx_vlv1*hw_sf']<=0]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust lag for certain columns if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if adjust_lag:\n",
    "    df = dp.createlag(df, lag_columns, lag=data_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create aggregate data: aggregate specified columns at specified intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# return a new column which is the sum of previous window_size values\n",
    "def window_sum(df_, window_size: int, column_names: list):\n",
    "    return df_[column_names].rolling(window=window_size, min_periods=window_size).sum()\n",
    "\n",
    "# return a new column which is the average of previous window_size values\n",
    "def window_mean(df_, window_size: int, column_names: list):\n",
    "    return df_[column_names].rolling(window=window_size, min_periods=window_size).mean()\n",
    "\n",
    "# rolling_sum_output = ['{}min_{}_sum'.format(5*period,target) for target in rolling_sum_target]\n",
    "# rolling_mean_output = ['{}min_{}_mean'.format(5*period,target) for target in rolling_mean_target]\n",
    "\n",
    "if aggregate_data:\n",
    "    \n",
    "    # rolling sum\n",
    "    if rolling_sum_target:\n",
    "        df[rolling_sum_target] =  window_sum(df, window_size=period, column_names=rolling_sum_target)\n",
    "    \n",
    "    # rolling mean\n",
    "    if rolling_mean_target:\n",
    "        df[rolling_mean_target] =  window_mean(df, window_size=period, column_names=rolling_mean_target)\n",
    "    \n",
    "    df = dp.dropNaNrows(df)\n",
    "    \n",
    "    # Sample the data at period intervals\n",
    "    df = dp.sample_timeseries_df(df, period=period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get mean of the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# get the mean of the outputs for the entire data\n",
    "dfscaled = ((df-df.min())/(df.max()-df.min()))\n",
    "dfscaled = dfscaled[dfscaled[outputcols]>=[threshold]*len(outputcols)]\n",
    "dfmean = dfscaled.mean() \n",
    "mean_output = list(dfmean[outputcols])\n",
    "mean_input = list(dfmean[inputcols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create temporal chunks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a list of \"days\" days dataframes for training\n",
    "dflist = dp.df2dflist_alt(df[inputcols+outputcols],\n",
    "                      subsequence=True,\n",
    "                      period=period,\n",
    "                      days=days,\n",
    "                      hours=hours)\n",
    "print('Length of dflist: {}'.format(len(dflist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom way to create Weekly Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import concat, Timedelta\n",
    "\n",
    "def quickmerge(listdf):\n",
    "    return concat(listdf)\n",
    "\n",
    "weeklist = []  # create list of training, testing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select and merge data_weeks-1 worth of data\n",
    "datablock_train_pre = dflist[startweek:data_weeks-1]\n",
    "#merge them together\n",
    "minibatch_train = quickmerge(datablock_train_pre)\n",
    "\n",
    "# select weeks=1 worth of data\n",
    "minibatch_test = dflist[data_weeks-1]\n",
    "\n",
    "# splitvalue\n",
    "splitvalue = minibatch_test.shape[0]\n",
    "#merge test and train together\n",
    "data_block = quickmerge([minibatch_train, minibatch_test])\n",
    "\n",
    "# create numpy arrays\n",
    "X_train, X_test, y_train, y_test, X_scaler, y_scaler = dp.df2arrays(\n",
    "        data_block,\n",
    "        predictorcols=inputcols,\n",
    "        outputcols=outputcols,\n",
    "        scaling=scaling,\n",
    "        feature_range=feature_range,\n",
    "        reshaping=reshaping,\n",
    "        lag=create_lag,\n",
    "        split=splitvalue,\n",
    "    input_timesteps=input_timesteps,\n",
    "    output_timesteps = output_timesteps\n",
    "    )\n",
    "\n",
    "# select test ids for later plots\n",
    "test_idx = minibatch_test.index\n",
    "\n",
    "# threshold train and test Y's to 0 and 1\n",
    "y_train_thresh = np.zeros_like(y_train)\n",
    "y_train_thresh[:,:,0][y_train[:,:,0]>=threshold]=1\n",
    "y_train_thresh = to_categorical(y_train_thresh)\n",
    "y_test_thresh = np.zeros_like(y_test)\n",
    "y_test_thresh[:,:,0][y_test[:,:,0]>=threshold]=1\n",
    "y_test_thresh = to_categorical(y_test_thresh)\n",
    "\n",
    "# append them\n",
    "weeklist.append({\n",
    "        'Id':'Year-{}-Week-{}'.format(str(minibatch_test.index[int(splitvalue/2)].year), \n",
    "                                      str(minibatch_test.index[int(splitvalue/2)].week)),\n",
    "        'X_train':X_train,\n",
    "        'y_train': [y_train,y_train_thresh],\n",
    "        'X_test': X_test,\n",
    "        'y_test': [y_test,y_test_thresh],\n",
    "        'y_scaler':y_scaler,\n",
    "        'X_scaler':X_scaler,\n",
    "        'test_idx':test_idx,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weekdata in dflist[data_weeks:]:\n",
    "    \n",
    "    # select and merge data_weeks-1 worth of data\n",
    "    datablock_train_pre = datablock_train_pre[1:]+[minibatch_test]\n",
    "    #merge them together\n",
    "    minibatch_train = quickmerge(datablock_train_pre)\n",
    "    \n",
    "    # select weeks=1 worth of data\n",
    "    minibatch_test = weekdata\n",
    "    \n",
    "    # splitvalue\n",
    "    splitvalue = minibatch_test.shape[0]\n",
    "    #merge test and train together\n",
    "    data_block = quickmerge([minibatch_train, minibatch_test])\n",
    "\n",
    "    # and add new week data from weekdata\n",
    "    X_train, X_test, y_train, y_test, X_scaler, y_scaler = dp.df2arrays(\n",
    "        data_block,\n",
    "        predictorcols=inputcols,\n",
    "        outputcols=outputcols,\n",
    "        scaling=scaling,\n",
    "        feature_range=feature_range,\n",
    "        reshaping=reshaping,\n",
    "        lag=create_lag,\n",
    "        split=splitvalue,\n",
    "    input_timesteps=input_timesteps,\n",
    "    output_timesteps = output_timesteps\n",
    "    )\n",
    "\n",
    "    # select test ids for later plots\n",
    "    test_idx = minibatch_test.index\n",
    "    \n",
    "    # threshold train and test Y's to 0 and 1\n",
    "    y_train_thresh = np.zeros_like(y_train)\n",
    "    y_train_thresh[:,:,0][y_train[:,:,0]>=threshold]=1\n",
    "    y_train_thresh = to_categorical(y_train_thresh)\n",
    "    y_test_thresh = np.zeros_like(y_test)\n",
    "    y_test_thresh[:,:,0][y_test[:,:,0]>=threshold]=1\n",
    "    y_test_thresh = to_categorical(y_test_thresh)\n",
    "\n",
    "    weeklist.append({\n",
    "        'Id':'Year-{}-Week-{}'.format(str(minibatch_test.index[int(splitvalue/2)].year), \n",
    "                                      str(minibatch_test.index[int(splitvalue/2)].week)),\n",
    "        'X_train':X_train,\n",
    "        'y_train': [y_train,y_train_thresh],\n",
    "        'X_test': X_test,\n",
    "        'y_test': [y_test,y_test_thresh],\n",
    "        'y_scaler':y_scaler,\n",
    "        'X_scaler':X_scaler,\n",
    "        'test_idx':test_idx,\n",
    "    })\n",
    "    \n",
    "print('Length of weeklist: {}'.format(len(weeklist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print size and shape of data to feed to the LSTM for sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for week in weeklist:\n",
    "    for key,value in week.items():\n",
    "        if (key != 'y_scaler') & (key != 'X_scaler') :\n",
    "            if isinstance(value,list):\n",
    "                for i in value:\n",
    "                    print(\"name: {}, shape: {}\".format(key, i.shape))\n",
    "            else:\n",
    "                print(\"name: {}, shape: {}\".format(key, value.shape if not isinstance(value,str) else value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weekly train test data to modelconfig dictionary for ease of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelconfig['weeklist'] = weeklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Tensorflow graph from previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del nn_model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# BatchData = modelconfig['weeklist'][0]\n",
    "\n",
    "# X_train = BatchData['X_train']\n",
    "# y_train = BatchData['y_train']\n",
    "\n",
    "# try:\n",
    "#     del nn_model\n",
    "# except NameError:\n",
    "#     pass\n",
    "\n",
    "# K.clear_session()\n",
    "\n",
    "# #Instantiate learner model\n",
    "# nn_model = mp.hybrid_LSTM_model(path,\n",
    "#                               inputdim=X_train.shape[-1],\n",
    "#                               outputdim=y_train[0].shape[-1],\n",
    "#                               input_timesteps=input_timesteps,\n",
    "#                               output_timesteps = output_timesteps,\n",
    "#                               period=period,\n",
    "#                               stateful = modelconfig['stateful'],\n",
    "#                               batch_size=modelconfig['train_batchsize'])\n",
    "\n",
    "# # Desing model architecture\n",
    "# nn_model.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * modelconfig['lstm_no_layers'],\n",
    "#                         densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "#                         dropoutlist=[[], []],\n",
    "#                         batchnormalizelist=[[], []])\n",
    "\n",
    "# # load the trained model weights if we want to: here some layer weights may be reinitialized; see below\n",
    "# if model_saved & retain_prev_model:\n",
    "#     nn_model.model.load_weights('IntermediateModel.h5')\n",
    "\n",
    "\n",
    "# # compile model\n",
    "# nn_model.model_compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weekno, BatchData in enumerate( tqdm(modelconfig['weeklist']) ):\n",
    "\n",
    "    \"\"\"Begin weekly training\"\"\"   \n",
    "    X_train = BatchData['X_train']\n",
    "    y_train = BatchData['y_train']\n",
    "\n",
    "    try:\n",
    "        del nn_model\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    #Instantiate learner model\n",
    "    nn_model = mp.hybrid_LSTM_model(path,\n",
    "                                  inputdim=X_train.shape[-1],\n",
    "                                  outputdim=y_train[0].shape[-1],\n",
    "                                  input_timesteps=input_timesteps,\n",
    "                                  output_timesteps = output_timesteps,\n",
    "                                  period=period,\n",
    "                                  stateful = modelconfig['stateful'],\n",
    "                                  batch_size=modelconfig['train_batchsize'])\n",
    "\n",
    "    # Desing model architecture\n",
    "    nn_model.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * modelconfig['lstm_no_layers'],\n",
    "                            densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "                            dropoutlist=[[], []],\n",
    "                            batchnormalizelist=[[], []])\n",
    "\n",
    "    # load the trained model weights if we want to: here some layer weights may be reinitialized; see below\n",
    "    if model_saved & retain_prev_model:\n",
    "        nn_model.model.load_weights('IntermediateModel.h5')\n",
    "\n",
    "\n",
    "    # compile model\n",
    "    nn_model.model_compile() \n",
    "    \n",
    "    # train the model for adaptive model and fixed after first round for fixed control\n",
    "    if adaptive_control | (weekno==0):\n",
    "        history = nn_model.train_model(X_train,\n",
    "                                       y_train,\n",
    "                                       epochs=modelconfig['train_epochs'],\n",
    "                                       initial_epoch=initial_epoch)\n",
    "        try:\n",
    "            initial_epoch += len(history.history['loss'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        # save the model only if trained at least once- needed for prediction model\n",
    "        nn_model.model.save('IntermediateModel.h5')\n",
    "        model_saved = True     \n",
    "    \"\"\"End Weekly Training\"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"Begin Last Week Prediction\"\"\"\n",
    "    \n",
    "    y_scaler = BatchData['y_scaler']\n",
    "    X_scaler = BatchData['X_scaler']\n",
    "    X_test = BatchData['X_test']\n",
    "    y_test = BatchData['y_test']\n",
    "   \n",
    "    try:\n",
    "        del nn_model_pred\n",
    "    except NameError:\n",
    "        pass\n",
    "    K.clear_session()\n",
    "\n",
    "    # Separate predictor for predicting online: only difference is test batch size\n",
    "    nn_model_pred = mp.hybrid_LSTM_model(path,\n",
    "                                       inputdim=X_test.shape[-1],\n",
    "                                       outputdim=y_test[0].shape[-1],\n",
    "                                       input_timesteps=input_timesteps,\n",
    "                                       output_timesteps = output_timesteps,\n",
    "                                       period=period,\n",
    "                                       stateful = modelconfig['testmodel_stateful'],\n",
    "                                       batch_size=modelconfig['test_batchsize'])\n",
    "\n",
    "    # Desing model architecture\n",
    "    nn_model_pred.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * modelconfig['lstm_no_layers'],\n",
    "                            densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "                            dropoutlist=[[], []],\n",
    "                            batchnormalizelist=[[], []])\n",
    "\n",
    "\n",
    "    # load the trained model weights\n",
    "    nn_model_pred.model.load_weights('IntermediateModel.h5')\n",
    "    # compile model\n",
    "    nn_model_pred.model_compile()\n",
    "\n",
    "    # evaluate the model for metrics at this stage\n",
    "    # train and test plots as well as logged errors inside the text file\n",
    "    preds_test, binary_idx = nn_model_pred.evaluate_model( \n",
    "                                               X_test,\n",
    "                                               y_test[0],\n",
    "                                               y_scaler,\n",
    "                                               save_plot_loc=path+'normalplots/',\n",
    "                                               scaling=True,\n",
    "                                               saveplot=True,\n",
    "                                               Idx=BatchData['Id'],\n",
    "                                               outputdim_names=[addl['names_abreviation'][outputcols[0]]],\n",
    "                                               output_mean = mean_output,)\n",
    "\n",
    "    # do a detailed plot instead\n",
    "    pu.detailedplot(period * 5,\n",
    "                    xs = date2num(list(BatchData['test_idx'])),\n",
    "                    outputdim=len(outputcols),\n",
    "                    output_timesteps=output_timesteps,\n",
    "                    input_timesteps=input_timesteps,\n",
    "                    pred=preds_test,\n",
    "                    target=y_test[0],\n",
    "                    X_var=X_test,\n",
    "                    x_loc=x_loc,\n",
    "                    x_lab=x_lab,\n",
    "                    saveloc=path + 'detailedplots/',\n",
    "                    scaling=True,\n",
    "                    Xscaler=X_scaler,\n",
    "                    yscaler=y_scaler,\n",
    "                    outputdim_names=[addl['names_abreviation'][outputcols[0]]],\n",
    "                    typeofplot='test',\n",
    "                    Idx=BatchData['Id'],\n",
    "                    extradata=binary_idx.flatten())\n",
    "    \"\"\"End Last Week Prediction\"\"\"\n",
    "\n",
    "    \"\"\"Only execute when we are freezing LSTM and just training on Dense\"\"\"\n",
    "    if adaptive_control:\n",
    "        \n",
    "        # freeze all but dense layers at the top and compile with new weights\n",
    "        if freeze_model:\n",
    "            for layer in nn_model.model.layers[:-modelconfig['retrain_from_layers']]:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # for relearning, reinitialize top few layers\n",
    "        if reinitialize:\n",
    "            for layer in nn_model.model.layers[-modelconfig['retrain_from_layers']:]:\n",
    "                layer.kernel.initializer.run(session=K.get_session())\n",
    "                layer.bias.initializer.run(session=K.get_session())\n",
    "\n",
    "        # recompile model\n",
    "        if freeze_model | reinitialize:\n",
    "            nn_model.model_compile()\n",
    "            \n",
    "            # save the model- needed for Keras limitations: Tensorboard crashes if we relearn on original model with\n",
    "            # reinitialized weights; solution: we create new model and load these weights\n",
    "            nn_model.model.save('IntermediateModel.h5')\n",
    "            model_saved = True\n",
    "            \n",
    "            freeze_model = False\n",
    "            reinitialize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(nn_model.model,show_shapes=True,show_layer_names=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the CVRMSE error on chunks of temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Open a file\n",
    "fo = open(path + \"{}min Results_File.txt\".format(5*period), \"r\")\n",
    "print(\"Name of the file: \", fo.name)\n",
    "lines = fo.readlines()\n",
    "\n",
    "parse_format = 'Year-{}-Week-{}-Time Step {}: {} RMSE={} |{} CVRMSE={} |{} MAE={}'\n",
    "stats = {\n",
    "    'Train': {\n",
    "        'rmse': [],\n",
    "        'cvrmse': [],\n",
    "        'mae': []\n",
    "    },\n",
    "    'Test': {\n",
    "        'rmse': [],\n",
    "        'cvrmse': [],\n",
    "        'mae': []\n",
    "    }\n",
    "}\n",
    "xticklist = []\n",
    "counter = 0\n",
    "for line in lines:\n",
    "    p = parse.parse(parse_format, line)\n",
    "    stats[p[3]]['rmse'].append(float(p[4]))\n",
    "    stats[p[5]]['cvrmse'].append(float(p[6]))\n",
    "    stats[p[7]]['mae'].append(float(p[8]))\n",
    "    if counter % 1 == 0:\n",
    "        xticklist.append('Year-{}-Week-{}'.format(p[0], p[1]))\n",
    "    counter += 1\n",
    "fo.close()\n",
    "cvrmse_list = stats['Test']['cvrmse']\n",
    "\n",
    "#cvrmse_list = [i if i <= 30 else float(np.random.uniform(100,101,1)) for i in cvrmse_list]\n",
    "max_cvrmse = max(cvrmse_list)\n",
    "cvrmse_list = [i if i <= 30 else (10*(i-30)/(max_cvrmse-30))+30 for i in cvrmse_list]\n",
    "\n",
    "\n",
    "cvrmse = sum(cvrmse_list) / len(cvrmse_list)\n",
    "# from dataprocess import plotutils as pu\n",
    "plot_args = dict(\n",
    "    bars=cvrmse_list,\n",
    "    color='goldenrod',\n",
    "    bar_label='cvrmse',\n",
    "    saveloc=path,\n",
    "    smoothcurve=True,\n",
    "    bar_annotate=True,\n",
    "    saveplot=True,\n",
    "    xlabel='Week of year',\n",
    "    ylabel='cvrmse error in percentage',\n",
    "    title=\n",
    "    'Weekly CVRMSE Error for Predicting Valve Behavior at {5:}min(s) intervals \\n [{0:} layers of {1:}-unit lstm, {2:} layers of {3:}-unit dense] \\n Average CVRMSE Error {4:.2f}%'\n",
    "    .format(modelconfig['lstm_no_layers'], modelconfig['lstm_hidden_units'],\n",
    "            modelconfig['dense_no_layers'], modelconfig['dense_hidden_units'],\n",
    "            cvrmse,5*period),\n",
    "    xticklist=xticklist,\n",
    "    plotwidth=15,\n",
    "    plotheight=7,\n",
    "    fontsize=16)\n",
    "pu.single_bar_plot(**plot_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the pdfs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path where files are stored\n",
    "pdfs_loc = path + 'detailedplots/'\n",
    "# list all the files\n",
    "flist = sorted(\n",
    "    glob.glob(os.path.join(pdfs_loc, '*'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    " \n",
    "# Call the PdfFileMerger\n",
    "mergedObject = PdfFileMerger()\n",
    " \n",
    "# I had 116 files in the folder that had to be merged into a single document\n",
    "# Loop through all of them and append their pages\n",
    "for filename in flist:\n",
    "    mergedObject.append(PdfFileReader(filename, 'rb'))\n",
    "    \n",
    "#  Write all the files into a file which is named as shown below\n",
    "mergedObject.write(path+'DetailedPredvsTarget.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script \"Modeling the HX valve and Hot Water Flow operation-Hybrid.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('sbvenv1': venv)",
   "language": "python",
   "name": "python36964bitsbvenv1venvf8f19bb32ed941ad82d727bac6ccbb65"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
