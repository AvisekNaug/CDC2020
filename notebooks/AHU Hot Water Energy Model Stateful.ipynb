{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Hot Water Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the parameters for the experiment and log in the experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile param_sets.json\n",
    "seed_value = 123  # seed for the experiment\n",
    "Trial = 0  # number of the experiment\n",
    "\n",
    "period = 6  # the period to sample the data at. 1 period= 5 minutes\n",
    "\n",
    "inputcols = ['oat', 'orh', 'sat', 'ghi', 'hw_sf', 'hw_st',\n",
    "             'hx_vlv1']  # input predictors\n",
    "outputcols = ['hwe']  # output targets\n",
    "input_timesteps = 1  # number of timesteps for the input sequence\n",
    "output_timesteps = 1  # number of timesteps for the output sequence\n",
    "\n",
    "# Smoothing\n",
    "smooth_data = True  # whetter to smooth the data or not\n",
    "order = 5  # order of the filer\n",
    "T = 300  # sampling_period in seconds\n",
    "fs = 1 / 300  # sample rate, Hz\n",
    "cutoff = 0.0001  # desired cutoff frequency of the filter, Hz\n",
    "\n",
    "# adjust out of phase data\n",
    "adjust_lag = True  # whether to adjust the lag for certain columns\n",
    "lag_columns = outputcols  # choose columns to adjust lag\n",
    "data_lag = -1  # lag by how many periods: negative means shift column upwards\n",
    "\n",
    "# aggregate data based on period\n",
    "aggregate_data = True  # aggregate data or not\n",
    "rolling_sum_target = ['hwe', 'hw_sf']  # create sum aggregate for these columns\n",
    "rolling_mean_target = [\n",
    "    'hw_st', 'hw_rt', 'oat', 'sat', 'orh', 'ghi', 'avg_stpt', 'hx_vlv1'\n",
    "]  # create mean aggregate for these columns\n",
    "\n",
    "# create temporal batches of data: df2dflist\n",
    "days, hours = 7, 0\n",
    "\n",
    "# Custom way to create Training Data\n",
    "data_weeks = 52  # Create a large initial block 6 months ~ 26 weeks of training and testing data\n",
    "create_lag = 0  # Create further lags in the output\n",
    "scaling = True  # Scale the input and output features\n",
    "feature_range = (0, 1)  # Scaling range\n",
    "reshaping = True  # reshape data according to (batch_size, time_steps, features)\n",
    "\n",
    "# model configuration\n",
    "modelconfig = {\n",
    "    'lstm_hidden_units': 4,\n",
    "    'lstm_no_layers': 2,\n",
    "    'dense_hidden_units': 8,\n",
    "    'dense_no_layers': 4,\n",
    "    'train_epochs': 5000,\n",
    "    'retrain_from_layers': 5,\n",
    "    'stateful': True\n",
    "}\n",
    "\n",
    "# wheter doing adaptive for fixed learning\n",
    "adaptive_control = True  # whether we relearn or keep it fixed\n",
    "path = '../results/' + outputcols[0] + '_model{}/'.format(\n",
    "    Trial) + 'adaptive/' * adaptive_control + 'fixed/' * (1 - adaptive_control)\n",
    "#!rm -rf ../results/lstm_hwe_trial8/adaptive\n",
    "\n",
    "#model design considerations\n",
    "modeldesigndone = False  # whether model will be reinitialized\n",
    "initial_epoch = 0  # the start epoch number for the training\n",
    "\n",
    "# These are automatically superseded and ignored if adaptive_control is set to False\n",
    "freeze_model = True  # freeze weights of certain layers\n",
    "reinitialize = True  # reinitialize the weights of certain layers\n",
    "model_saved = False  # whether model has been saved once\n",
    "\n",
    "# data used for learning the model\n",
    "datapath = '../data/processed/interpolated/buildingdata_interpolated.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed in numpy, Keras and TF for reproducability\n",
    "### Import modules\n",
    "### Set GPU configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Enable '0' or disable '-1' GPU use\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# including the project directory to the notebook level\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import parse\n",
    "import warnings\n",
    "from matplotlib.dates import date2num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    import tensorflow as tf\n",
    "    #tf.random.set_seed(seed_value)\n",
    "    # for later versions: \n",
    "    tf.compat.v1.set_random_seed(seed_value)\n",
    "    config = tf.ConfigProto(log_device_placement=False)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    \n",
    "    from keras import backend as K\n",
    "    from nn_source import models as mp\n",
    "\n",
    "from dataprocess import dataprocessor as dp\n",
    "from dataprocess import plotutils as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir '../results/lstm_hwe_trial1/loginfo/' --port 8200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folder to save models and tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# create the results directory\n",
    "try:\n",
    "    os.makedirs(path)\n",
    "except FileExistsError:\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        try:\n",
    "            shutil.rmtree(path + f)\n",
    "        except NotADirectoryError:\n",
    "            os.remove(path + f)\n",
    "        \n",
    "os.mkdir(path + 'loginfo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the experiment parameters and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#save the values\n",
    "paramsdict = {\n",
    "    \n",
    "    'seed_value' : seed_value,\n",
    "    \n",
    "    'period':period,\n",
    "    \n",
    "    'inputcols':inputcols,\n",
    "    'outputcols':outputcols,\n",
    "    'input_timesteps':input_timesteps,\n",
    "    'output_timesteps':output_timesteps,\n",
    "    \n",
    "    'smooth_data': smooth_data,\n",
    "    'order' : 5,\n",
    "    'T' : T,\n",
    "    'fs' : fs,\n",
    "    'cutoff' : cutoff,\n",
    "    \n",
    "    'adjust_lag' : adjust_lag,\n",
    "    'lag_columns' : lag_columns,\n",
    "    'data_lag' : data_lag,\n",
    "    \n",
    "    'aggregate_data' : aggregate_data,\n",
    "    'rolling_sum_target' : rolling_sum_target,\n",
    "    'rolling_mean_target' : rolling_mean_target,\n",
    "    \n",
    "    'days':days,\n",
    "    'hours':hours,\n",
    "    \n",
    "    'data_weeks' : data_weeks,\n",
    "    'create_lag' : create_lag,\n",
    "    'scaling' : scaling,\n",
    "    'feature_range' : feature_range,\n",
    "    'reshaping' : reshaping,\n",
    "    \n",
    "    'modelconfig' : modelconfig,\n",
    "    \n",
    "    'adaptive_control':adaptive_control,\n",
    "    'path':path,\n",
    "    \n",
    "    'modeldesigndone' : modeldesigndone,\n",
    "    'initial_epoch' : initial_epoch,\n",
    "    'freeze_model' : freeze_model,\n",
    "    'reinitialize' : reinitialize,\n",
    "    'model_saved' : model_saved,\n",
    "    \n",
    "    'datapath' : datapath,\n",
    "    \n",
    "    \n",
    "}\n",
    "    \n",
    "# with open(path+'params.json', 'r') as fp:\n",
    "#     param2dict = json.load(fp)\n",
    "\n",
    "with open(path+'params.json', 'w') as fp:\n",
    "    json.dump(paramsdict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oat</th>\n",
       "      <th>sat</th>\n",
       "      <th>orh</th>\n",
       "      <th>hwe</th>\n",
       "      <th>cwe</th>\n",
       "      <th>ghi</th>\n",
       "      <th>avg_stpt</th>\n",
       "      <th>flow</th>\n",
       "      <th>hw_sf</th>\n",
       "      <th>hw_st</th>\n",
       "      <th>hw_rt</th>\n",
       "      <th>hx_vlv1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-13 07:50:00</th>\n",
       "      <td>80.913567</td>\n",
       "      <td>79.320808</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.546185</td>\n",
       "      <td>562.0</td>\n",
       "      <td>69.52381</td>\n",
       "      <td>39.534317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.427994</td>\n",
       "      <td>89.589584</td>\n",
       "      <td>0.276543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-13 07:55:00</th>\n",
       "      <td>80.913567</td>\n",
       "      <td>79.023285</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.293947</td>\n",
       "      <td>580.0</td>\n",
       "      <td>69.52381</td>\n",
       "      <td>39.809956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.125771</td>\n",
       "      <td>89.589584</td>\n",
       "      <td>0.276543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-13 08:00:00</th>\n",
       "      <td>80.615402</td>\n",
       "      <td>79.023285</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.685425</td>\n",
       "      <td>597.0</td>\n",
       "      <td>69.52381</td>\n",
       "      <td>39.174774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.125771</td>\n",
       "      <td>89.589584</td>\n",
       "      <td>0.276543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-13 08:05:00</th>\n",
       "      <td>80.303123</td>\n",
       "      <td>79.023285</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.074446</td>\n",
       "      <td>614.0</td>\n",
       "      <td>69.52381</td>\n",
       "      <td>35.745533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.816162</td>\n",
       "      <td>89.229790</td>\n",
       "      <td>0.276543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-13 08:10:00</th>\n",
       "      <td>80.303123</td>\n",
       "      <td>79.023285</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.646845</td>\n",
       "      <td>625.0</td>\n",
       "      <td>69.52381</td>\n",
       "      <td>34.858036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.816162</td>\n",
       "      <td>89.229790</td>\n",
       "      <td>0.276543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           oat        sat   orh  hwe        cwe    ghi  \\\n",
       "Time                                                                     \n",
       "2018-07-13 07:50:00  80.913567  79.320808  82.0  0.0  32.546185  562.0   \n",
       "2018-07-13 07:55:00  80.913567  79.023285  82.0  0.0  29.293947  580.0   \n",
       "2018-07-13 08:00:00  80.615402  79.023285  82.0  0.0  30.685425  597.0   \n",
       "2018-07-13 08:05:00  80.303123  79.023285  82.0  0.0  30.074446  614.0   \n",
       "2018-07-13 08:10:00  80.303123  79.023285  79.0  0.0  29.646845  625.0   \n",
       "\n",
       "                     avg_stpt       flow  hw_sf      hw_st      hw_rt  \\\n",
       "Time                                                                    \n",
       "2018-07-13 07:50:00  69.52381  39.534317    0.0  94.427994  89.589584   \n",
       "2018-07-13 07:55:00  69.52381  39.809956    0.0  94.125771  89.589584   \n",
       "2018-07-13 08:00:00  69.52381  39.174774    0.0  94.125771  89.589584   \n",
       "2018-07-13 08:05:00  69.52381  35.745533    0.0  93.816162  89.229790   \n",
       "2018-07-13 08:10:00  69.52381  34.858036    0.0  93.816162  89.229790   \n",
       "\n",
       "                      hx_vlv1  \n",
       "Time                           \n",
       "2018-07-13 07:50:00  0.276543  \n",
       "2018-07-13 07:55:00  0.276543  \n",
       "2018-07-13 08:00:00  0.276543  \n",
       "2018-07-13 08:05:00  0.276543  \n",
       "2018-07-13 08:10:00  0.276543  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the pickled file for ahu data\n",
    "dfdata = dp.readfile(datapath)\n",
    "\n",
    "# return pickled df\n",
    "df = dfdata.return_df(processmethods=['file2df'])\n",
    "\n",
    "# show data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if smooth_data:\n",
    "    df = dp.dfsmoothing(df=df,\n",
    "                        column_names=list(df.columns),\n",
    "                        order=order,\n",
    "                        Wn=cutoff,\n",
    "                        T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust lag for certain columns if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if adjust_lag:\n",
    "    df = dp.createlag(df, lag_columns, lag=data_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create aggregate data: aggregate specified columns at specified intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# return a new column which is the sum of previous window_size values\n",
    "def window_sum(df_, window_size: int, column_names: list):\n",
    "    return df_[column_names].rolling(window=window_size, min_periods=window_size).sum()\n",
    "\n",
    "# return a new column which is the average of previous window_size values\n",
    "def window_mean(df_, window_size: int, column_names: list):\n",
    "    return df_[column_names].rolling(window=window_size, min_periods=window_size).mean()\n",
    "\n",
    "# rolling_sum_output = ['{}min_{}_sum'.format(5*period,target) for target in rolling_sum_target]\n",
    "# rolling_mean_output = ['{}min_{}_mean'.format(5*period,target) for target in rolling_mean_target]\n",
    "\n",
    "if aggregate_data:\n",
    "    \n",
    "    # rolling sum\n",
    "    df[rolling_sum_target] =  window_sum(df, window_size=period, column_names=rolling_sum_target)\n",
    "    \n",
    "    # rolling mean\n",
    "    df[rolling_mean_target] =  window_mean(df, window_size=period, column_names=rolling_mean_target)\n",
    "    \n",
    "    df = dp.dropNaNrows(df)\n",
    "    \n",
    "    # Sample the data at period intervals\n",
    "    df = dp.sample_timeseries_df(df, period=period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create temporal chunks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dflist: 82\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of \"days\" day dataframes for training\n",
    "dflist = dp.df2dflist_alt(df[inputcols+outputcols],\n",
    "                      subsequence=True,\n",
    "                      period=period,\n",
    "                      days=days,\n",
    "                      hours=hours)\n",
    "print('Length of dflist: {}'.format(len(dflist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom way to create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Since we are training across time, we need to ensure the input and output time steps are now 1\n",
    "assert (input_timesteps == 1) & (\n",
    "    output_timesteps == 1), \"Input and Output timesteps must be 1 for this notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create placeholder and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from pandas import concat, Timedelta\n",
    "\n",
    "\n",
    "def quickmerge(listdf):\n",
    "    return concat(listdf)\n",
    "\n",
    "\n",
    "def df_list_reshape(datablock_df):\n",
    "    new_df_list = []\n",
    "    for i in range(int(10080 / (5 * period))):\n",
    "        new_df_list.append(\n",
    "            datablock_df[(datablock_df.index - datablock_df.index[i]) %\n",
    "                         Timedelta('7 days') == '0 days 00:00:00'])\n",
    "    return new_df_list\n",
    "\n",
    "weeklist = []  # create list of training, testing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Data Block for offline training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "datablock_pre = dflist[0:data_weeks-1]\n",
    "datablock_train = quickmerge(datablock_pre)\n",
    "datablock_train = quickmerge(df_list_reshape(datablock_train))\n",
    "datablock_test = dflist[data_weeks-1]\n",
    "data_block = quickmerge([datablock_train, datablock_test])\n",
    "splitvalue = 336   # (data_weeks - 1) / data_weeks  # One week for training\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_scaler, y_scaler = dp.df2arrays(\n",
    "        data_block,\n",
    "        predictorcols=inputcols,\n",
    "        outputcols=outputcols,\n",
    "        scaling=scaling,\n",
    "        feature_range=feature_range,\n",
    "        reshaping=reshaping,\n",
    "        lag=create_lag,\n",
    "        split=splitvalue,\n",
    "    input_timesteps=input_timesteps,\n",
    "    output_timesteps = output_timesteps\n",
    "    )\n",
    "\n",
    "idx_end = -max(X_test.shape[1],y_test.shape[1])\n",
    "idx_start = idx_end - X_test.shape[0] + 1\n",
    "test_idx = data_block.index[[i for i in range(idx_start, idx_end+1, 1)]]\n",
    "\n",
    "weeklist.append({\n",
    "        'Id':'Year-{}-Week-{}'.format(str(datablock_test.index[0].year), \n",
    "                                      str(datablock_test.index[0].week)),\n",
    "        'X_train':X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_scaler':y_scaler,\n",
    "        'X_scaler':X_scaler,\n",
    "        'test_idx':test_idx,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create small blocks of data for weekly training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of weeklist: 31\n"
     ]
    }
   ],
   "source": [
    "for weekdata in dflist[data_weeks:]:\n",
    "    \n",
    "    datablock_pre = datablock_pre[1:]+[datablock_test]\n",
    "    datablock_train = quickmerge(datablock_pre)\n",
    "    datablock_train = quickmerge(df_list_reshape(datablock_train))\n",
    "    datablock_test = weekdata\n",
    "    data_block = quickmerge([datablock_train, datablock_test])\n",
    "    splitvalue = 336   # (data_weeks - 1) / data_weeks  # One week for training\n",
    "    \n",
    "    # and add new week data from weekdata\n",
    "    X_train, X_test, y_train, y_test, X_scaler, y_scaler = dp.df2arrays(\n",
    "        data_block,\n",
    "        predictorcols=inputcols,\n",
    "        outputcols=outputcols,\n",
    "        scaling=scaling,\n",
    "        feature_range=feature_range,\n",
    "        reshaping=reshaping,\n",
    "        lag=create_lag,\n",
    "        split=splitvalue,\n",
    "    input_timesteps=input_timesteps,\n",
    "    output_timesteps = output_timesteps\n",
    "    )\n",
    "\n",
    "    idx_end = -max(X_test.shape[1],y_test.shape[1])\n",
    "    idx_start = idx_end - X_test.shape[0] + 1\n",
    "    test_idx = data_block.index[[i for i in range(idx_start,idx_end+1,1)]]\n",
    "\n",
    "    weeklist.append({\n",
    "        'Id':'Year-{}-Week-{}'.format(str(datablock_test.index[-1].year), \n",
    "                                      str(datablock_test.index[-1].week)),\n",
    "        'X_train':X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_scaler':y_scaler,\n",
    "        'X_scaler':X_scaler,\n",
    "        'test_idx':test_idx,\n",
    "    })\n",
    "    \n",
    "print('Length of weeklist: {}'.format(len(weeklist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print size and shape of data to feed to the LSTM for sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Id, value: Year-2019-Week-27\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-29\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-30\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-31\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-32\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-33\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-34\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-35\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-36\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-37\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-38\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-39\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-40\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-41\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-42\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-43\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-44\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-45\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-46\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-47\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-48\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-49\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-50\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-51\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2019-Week-52\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2020-Week-1\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2020-Week-2\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2020-Week-3\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2020-Week-4\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2020-Week-5\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n",
      "name: Id, value: Year-2020-Week-6\n",
      "name: X_train, value: (17136, 1, 7)\n",
      "name: y_train, value: (17136, 1, 1)\n",
      "name: X_test, value: (336, 1, 7)\n",
      "name: y_test, value: (336, 1, 1)\n",
      "name: test_idx, value: (336,)\n"
     ]
    }
   ],
   "source": [
    "for week in weeklist:\n",
    "    for key,value in week.items():\n",
    "        if (key != 'y_scaler') & (key != 'X_scaler') :\n",
    "            print(\"name: {}, value: {}\".format(key, value.shape if not isinstance(value,str) else value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add weekly train test data to modelconfig dictionary for ease of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelconfig['weeklist'] = weeklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  Clear the Tensorflow graph from previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del nn_model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# weekdata = modelconfig['weeklist'][0]\n",
    "# X_train = weekdata['X_train']\n",
    "# y_train = weekdata['y_train']\n",
    "# X_test = weekdata['X_test']\n",
    "# y_test = weekdata['y_test']\n",
    "# y_scaler = weekdata['y_scaler']\n",
    "\n",
    "# #Instantiate learner model\n",
    "# nn_model = mp.seq2seq_model(path,\n",
    "#                       inputdim=X_train.shape[-1],\n",
    "#                       outputdim=y_train.shape[-1],\n",
    "#                       period=period)\n",
    "\n",
    "# # Desing model architecture\n",
    "# nn_model.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * (2*modelconfig['lstm_no_layers']),\n",
    "#                    densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "#                    dropoutlist=[[], []],\n",
    "#                    batchnormalizelist=[[], []])\n",
    "\n",
    "# # compile model\n",
    "# nn_model.model_compile()\n",
    "\n",
    "# nn_model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Perform relearning on chunks of temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for weekno, weekdata in enumerate(tqdm(modelconfig['weeklist'])):\n",
    "\n",
    "    X_train = weekdata['X_train']\n",
    "    y_train = weekdata['y_train']\n",
    "    X_test = weekdata['X_test']\n",
    "    y_test = weekdata['y_test']\n",
    "    y_scaler = weekdata['y_scaler']\n",
    "    X_scaler = weekdata['X_scaler']\n",
    "\n",
    "    if not modeldesigndone:\n",
    "\n",
    "        try:\n",
    "            del nn_model\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        K.clear_session()\n",
    "\n",
    "        #Instantiate learner model\n",
    "        nn_model = mp.ts_seq2seq_model(path,\n",
    "                                       inputdim=X_train.shape[-1],\n",
    "                                       outputdim=y_train.shape[-1],\n",
    "                                       input_timesteps=X_train.shape[-2],\n",
    "                                       period=period)\n",
    "\n",
    "        # Desing model architecture\n",
    "        nn_model.design_network(\n",
    "            lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * (2 * modelconfig['lstm_no_layers']),\n",
    "            densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "            dropoutlist=[[], []], batchnormalizelist=[[], []])\n",
    "        \n",
    "        if model_saved:\n",
    "            nn_model.model.load_weights('IntermediateModel.h5')\n",
    "        \n",
    "\n",
    "        # compile model\n",
    "        nn_model.model_compile()\n",
    "\n",
    "        # creating early stopping and learning reate changing callbacks\n",
    "        # nn_model.model_callbacks()\n",
    "\n",
    "        # modeldesigndone = True: needed for Keras limitations: Tensorboard crashes if we relearn on original model\n",
    "\n",
    "    # train the model for adaptive model and fixed after first round for fixed model\n",
    "    if adaptive_control | (weekno==0):\n",
    "        history = nn_model.train_model(X_train,\n",
    "                                       y_train,\n",
    "                                       X_test,\n",
    "                                       y_test,\n",
    "                                       epochs=modelconfig['train_epochs'],\n",
    "                                       initial_epoch=initial_epoch)\n",
    "        try:\n",
    "            initial_epoch += len(history.history['loss'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # evaluate the model for metrics at this stage\n",
    "    # train and test plots as well as logged errors inside the text file\n",
    "    preds_train, preds_test = nn_model.evaluate_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        y_scaler,\n",
    "        scaling=True,\n",
    "        saveplot=True,\n",
    "        Idx=weekdata['Id'],\n",
    "        outputdim_names=outputcols)\n",
    "    \n",
    "    # do a detailed plot instead\n",
    "    pu.detailedplot(period * 5,\n",
    "                    xs = date2num(list(weekdata['test_idx'])),\n",
    "                    outputdim=len(outputcols),\n",
    "                    output_timesteps=output_timesteps,\n",
    "                    input_timesteps=input_timesteps,\n",
    "                    pred=preds_test,\n",
    "                    target=y_test,\n",
    "                    X_var=X_test,\n",
    "                    saveloc=path,\n",
    "                    scaling=True,\n",
    "                    Xscaler=X_scaler,\n",
    "                    yscaler=y_scaler,\n",
    "                    typeofplot='test',\n",
    "                    Idx=weekdata['Id'])\n",
    "\n",
    "    if adaptive_control:\n",
    "        \n",
    "        # freeze all but dense layers at the top and compile with new weights\n",
    "        if freeze_model:\n",
    "            for layer in nn_model.model.layers[:-modelconfig['retrain_from_layers']]:\n",
    "                layer.trainable = False\n",
    "            freeze_model = False\n",
    "\n",
    "        # for relearning, reinitialize top few layers\n",
    "        if reinitialize:\n",
    "            for layer in nn_model.model.layers[\n",
    "                    -modelconfig['retrain_from_layers']:]:\n",
    "                layer.kernel.initializer.run(session=K.get_session())\n",
    "                layer.bias.initializer.run(session=K.get_session())\n",
    "\n",
    "        # recompile model\n",
    "        nn_model.model_compile()\n",
    "\n",
    "        # save the model- needed for Keras limitations: Tensorboard crashes if we relearn on original model\n",
    "        nn_model.model.save('IntermediateModel.h5')\n",
    "        model_saved = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plot the CVRMSE error on chunks of temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Open a file\n",
    "fo = open(path + \"{}min Results_File.txt\".format(5*period), \"r\")\n",
    "print(\"Name of the file: \", fo.name)\n",
    "lines = fo.readlines()\n",
    "\n",
    "parse_format = 'Year-{}-Week-{}-Time Step {}: {} RMSE={} |{} CVRMSE={} |{} MAE={}'\n",
    "stats = {\n",
    "    'Train': {\n",
    "        'rmse': [],\n",
    "        'cvrmse': [],\n",
    "        'mae': []\n",
    "    },\n",
    "    'Test': {\n",
    "        'rmse': [],\n",
    "        'cvrmse': [],\n",
    "        'mae': []\n",
    "    }\n",
    "}\n",
    "xticklist = []\n",
    "counter = 0\n",
    "for line in lines:\n",
    "    p = parse.parse(parse_format, line)\n",
    "    stats[p[3]]['rmse'].append(float(p[4]))\n",
    "    stats[p[5]]['cvrmse'].append(float(p[6]))\n",
    "    stats[p[7]]['mae'].append(float(p[8]))\n",
    "    if counter % 2 == 0:\n",
    "        xticklist.append('Year-{}-Week-{}'.format(p[0], p[1]))\n",
    "    counter += 1\n",
    "fo.close()\n",
    "cvrmse_list = stats['Test']['cvrmse']\n",
    "cvrmse = sum(cvrmse_list) / len(cvrmse_list)\n",
    "# from dataprocess import plotutils as pu\n",
    "plot_args = dict(\n",
    "    bars=cvrmse_list,\n",
    "    color='goldenrod',\n",
    "    bar_label='cvrmse',\n",
    "    saveloc=path,\n",
    "    smoothcurve=True,\n",
    "    bar_annotate=True,\n",
    "    saveplot=True,\n",
    "    xlabel='Week of year',\n",
    "    ylabel='cvrmse error in percentage',\n",
    "    title=\n",
    "    'Weekly CVRMSE Error for Hot Water Energy Model error at {5:}min(s) intervals \\n [{0:} layers of {1:}-unit lstm, {2:} layers of {3:}-unit dense] \\n Average CVRMSE Error {4:.2f}%'\n",
    "    .format(modelconfig['lstm_no_layers'], modelconfig['lstm_hidden_units'],\n",
    "            modelconfig['dense_no_layers'], modelconfig['dense_hidden_units'],\n",
    "            cvrmse,5*period),\n",
    "    xticklist=xticklist,\n",
    "    plotwidth=10,\n",
    "    plotheight=7,\n",
    "    fontsize=16)\n",
    "pu.single_bar_plot(**plot_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### Code cemetery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(nn_model.model, to_file='model.png',show_shapes=True,)\n",
    "\n",
    "\n",
    "# # Check the trainable status of the individual layers\n",
    "# for layer in nn_model.model.layers:\n",
    "#     print(layer, layer.trainable, layer.name)\n",
    "\n",
    "\n",
    "#model.model.summary()\n",
    "\n",
    "\n",
    "#nn_model.model.layers[-1].input_shape\n",
    "#model.model.layers[-1].kernel.initializer.run(session=K.get_session())\n",
    "#model.model.layers[-1].bias.initializer.run(session=K.get_session())\n",
    "#nn_model.model.layers[-1].output_shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('sbvenv1': venv)",
   "language": "python",
   "name": "python36964bitsbvenv1venv1a534851ebbc4d609aad5dcf7b359ab5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
