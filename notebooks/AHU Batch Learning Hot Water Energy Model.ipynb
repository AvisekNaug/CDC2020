{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the parameters for the experiment and log in the experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile param_sets.json\n",
    "seed_value = 123  # seed for the experiment\n",
    "Trial = 3  # number of the experiment\n",
    "\n",
    "period = 6  # the period to sample the data at. 1 period= 5 minutes\n",
    "\n",
    "inputcols = ['oat', 'orh', 'sat-oat', 'wbt']  # input predictors\n",
    "x_loc = [0, 1, 2, 3]  # vars we want to plot in detailed plot\n",
    "outputcols = ['hwe']  # output targets\n",
    "input_timesteps = 1  # number of timesteps for the input sequence\n",
    "output_timesteps = 1  # number of timesteps for the output sequence\n",
    "\n",
    "# Smoothing\n",
    "smooth_data = True  # whether to smooth the data or not\n",
    "order = 5  # order of the filer\n",
    "T = 300  # sampling_period in seconds\n",
    "fs = 1 / 300  # sample rate, Hz\n",
    "cutoff = 0.0001  # desired cutoff frequency of the filter, Hz\n",
    "\n",
    "# adjust out of phase data\n",
    "adjust_lag = True  # whether to adjust the lag for certain columns\n",
    "lag_columns = outputcols  # choose columns to adjust lag\n",
    "data_lag = -1  # lag by how many periods: negative means shift column upwards\n",
    "\n",
    "# aggregate data based on period\n",
    "aggregate_data = True  # aggregate data or not\n",
    "rolling_sum_target = ['hwe']  # create sum aggregate for these columns\n",
    "rolling_mean_target = [\n",
    "    'oat', 'orh', 'sat-oat', 'wbt',\n",
    "]  # create mean aggregate for these columns\n",
    "\n",
    "# create temporal batches of data: df2dflist\n",
    "days, hours = 7, 0\n",
    "\n",
    "# Values below which we consider as 0 output in hwe units is kBTU in period*5 mins\n",
    "threshold = 0.5\n",
    "\n",
    "# Custom way to create Training Data\n",
    "startweek = 0  # start week; indicates how large training set is\n",
    "data_weeks = 39  # Create a large initial block startweek-data_weeks weeks of training and testing data\n",
    "end_week = -1\n",
    "create_lag = 0  # Create further lags in the output\n",
    "scaling = True  # Scale the input and output features\n",
    "feature_range = (0, 1)  # Scaling range\n",
    "reshaping = True  # reshape data according to (batch_size, time_steps, features)\n",
    "\n",
    "# model configuration\n",
    "modelconfig = {\n",
    "    'lstm_hidden_units': 4,\n",
    "    'lstm_no_layers': 0,\n",
    "    'dense_hidden_units': 16,\n",
    "    'dense_no_layers': 6,\n",
    "    'retrain_from_layers': 3,\n",
    "    'train_stateful': False,\n",
    "    'test_stateful':False,\n",
    "    'train_batchsize':32,\n",
    "    'test_batchsize':1, # we are doing online prediction\n",
    "    'train_epochs': 5000,\n",
    "}\n",
    "\n",
    "# wheter doing adaptive for fixed learning\n",
    "adaptive_control = True  # whether we relearn or keep it fixed\n",
    "path = '../results/' + outputcols[0] + '_model_{}/'.format(\n",
    "    Trial) + 'adaptive/' * adaptive_control + 'fixed/' * (1 - adaptive_control)\n",
    "#!rm -rf ../results/lstm_hwe_trial8/adaptive\n",
    "\n",
    "#model design considerations\n",
    "modeldesigndone = False  # whether model will be reinitialized\n",
    "initial_epoch = 0  # the start epoch number for the training\n",
    "\n",
    "# These are automatically superseded and ignored if adaptive_control is set to False\n",
    "retain_prev_model = True  # retain weights of model from previous training\n",
    "freeze_model = True  # freeze weights of certain layers\n",
    "reinitialize = True  # reinitialize the weights of certain layers\n",
    "\n",
    "model_saved = False  # whether model has been saved once\n",
    "test_model_created = False  # create an idectical model for online predicton\n",
    "\n",
    "# data used for learning the model\n",
    "datapath = '../data/processed/buildingdata.pkl'\n",
    "\n",
    "# additional info\n",
    "addl = {\n",
    "    'metainfo': 'create a diff of sat and oat for hot water energy prediction as it is useful. See 1.0.8',\n",
    "    'names_abreviation': {\n",
    "        'oat':'Outside Air Temperature',\n",
    "        'orh':'Outside Air Relative Humidity',\n",
    "        'sat-oat' : 'Difference of Supply Air and Outside Air Temps ',\n",
    "        'ghi': 'Global Solar Irradiance',\n",
    "        'hw_sf':'Hot Water System Flow Rate',\n",
    "        'hx_vlv1':'Hot Water Valve %',\n",
    "        'hw_st':'Hot Water Supply Temperature',\n",
    "        'hwe': 'Hot Water Energy Consumption',\n",
    "        'wbt': 'Wet Builb Temperature',\n",
    "    }\n",
    "}\n",
    "x_lab = [addl['names_abreviation'][inputcols[i]] for i in x_loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Seed in numpy, Keras and TF for reproducability; Import modules; Set GPU configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Enable '0' or disable '-1' GPU use\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# including the project directory to the notebook level\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import parse\n",
    "import warnings\n",
    "from matplotlib.dates import date2num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    import tensorflow as tf\n",
    "    #tf.random.set_seed(seed_value)\n",
    "    # for later versions: \n",
    "    tf.compat.v1.set_random_seed(seed_value)\n",
    "    config = tf.ConfigProto(log_device_placement=False)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    \n",
    "    from keras import backend as K\n",
    "    from nn_source import models as mp\n",
    "\n",
    "from dataprocess import dataprocessor as dp\n",
    "from dataprocess import plotutils as pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir '../results/lstm_hwe_trial1/loginfo/' --port 8200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder to save models and tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# create the results directory\n",
    "try:\n",
    "    os.makedirs(path)\n",
    "except FileExistsError:\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        try:\n",
    "            shutil.rmtree(path + f)\n",
    "        except NotADirectoryError:\n",
    "            os.remove(path + f)\n",
    "        \n",
    "os.mkdir(path + 'loginfo')\n",
    "os.mkdir(path + 'normalplots')\n",
    "os.mkdir(path + 'detailedplots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the experiment parameters and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#save the values\n",
    "paramsdict = {\n",
    "    \n",
    "    'seed_value' : seed_value,\n",
    "    \n",
    "    'period':period,\n",
    "    \n",
    "    'inputcols':inputcols,\n",
    "    'x_loc': x_loc,\n",
    "    'outputcols':outputcols,\n",
    "    'input_timesteps':input_timesteps,\n",
    "    'output_timesteps':output_timesteps,\n",
    "    \n",
    "    'smooth_data': smooth_data,\n",
    "    'order' : 5,\n",
    "    'T' : T,\n",
    "    'fs' : fs,\n",
    "    'cutoff' : cutoff,\n",
    "    \n",
    "    'adjust_lag' : adjust_lag,\n",
    "    'lag_columns' : lag_columns,\n",
    "    'data_lag' : data_lag,\n",
    "    \n",
    "    'aggregate_data' : aggregate_data,\n",
    "    'rolling_sum_target' : rolling_sum_target,\n",
    "    'rolling_mean_target' : rolling_mean_target,\n",
    "    \n",
    "    'days':days,\n",
    "    'hours':hours,\n",
    "    \n",
    "    'threshold': threshold,\n",
    "    \n",
    "    'startweek': startweek,\n",
    "    'data_weeks' : data_weeks,\n",
    "    'end_week':end_week,\n",
    "    'create_lag' : create_lag,\n",
    "    'scaling' : scaling,\n",
    "    'feature_range' : feature_range,\n",
    "    'reshaping' : reshaping,\n",
    "    \n",
    "    'modelconfig' : modelconfig,\n",
    "    \n",
    "    'adaptive_control':adaptive_control,\n",
    "    'path':path,\n",
    "    \n",
    "    'modeldesigndone' : modeldesigndone,\n",
    "    'initial_epoch' : initial_epoch,\n",
    "    \n",
    "    'retain_prev_model' : retain_prev_model,\n",
    "    'freeze_model' : freeze_model,\n",
    "    'reinitialize' : reinitialize,\n",
    "    \n",
    "    'model_saved' : model_saved,\n",
    "    'test_model_created': test_model_created,\n",
    "    \n",
    "    'datapath' : datapath,\n",
    "    \n",
    "    'addl' : addl,\n",
    "}\n",
    "    \n",
    "# with open(path+'params.json', 'r') as fp:\n",
    "#     param2dict = json.load(fp)\n",
    "\n",
    "with open(path+'params.json', 'w') as fp:\n",
    "    json.dump(paramsdict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# read the pickled file for ahu data\n",
    "df1data = dp.readfile(datapath)\n",
    "\n",
    "# return pickled df\n",
    "df1 = df1data.return_df(processmethods=['file2df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# read the pickled file for wet bulb data\n",
    "df2data = dp.readfile('../data/processed/interpolated/wetbulbtemp.pkl')\n",
    "\n",
    "# return pickled df\n",
    "df2 = df2data.return_df(processmethods=['file2df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df = dp.merge_df_columns([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create additional Data columns as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df['sat-oat']= df['sat']-df['oat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if smooth_data:\n",
    "    df = dp.dfsmoothing(df=df,\n",
    "                        column_names=list(df.columns),\n",
    "                        order=order,\n",
    "                        Wn=cutoff,\n",
    "                        T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove <0 values for output energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "df['hwe'][df['hwe']<=0]=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust lag for certain columns if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if adjust_lag:\n",
    "    df = dp.createlag(df, lag_columns, lag=data_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create aggregate data: aggregate specified columns at specified intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# return a new column which is the sum of previous window_size values\n",
    "def window_sum(df_, window_size: int, column_names: list):\n",
    "    return df_[column_names].rolling(window=window_size, min_periods=window_size).sum()\n",
    "\n",
    "# return a new column which is the average of previous window_size values\n",
    "def window_mean(df_, window_size: int, column_names: list):\n",
    "    return df_[column_names].rolling(window=window_size, min_periods=window_size).mean()\n",
    "\n",
    "# rolling_sum_output = ['{}min_{}_sum'.format(5*period,target) for target in rolling_sum_target]\n",
    "# rolling_mean_output = ['{}min_{}_mean'.format(5*period,target) for target in rolling_mean_target]\n",
    "\n",
    "if aggregate_data:\n",
    "    \n",
    "    # rolling sum\n",
    "    if rolling_sum_target:\n",
    "        df[rolling_sum_target] =  window_sum(df, window_size=period, column_names=rolling_sum_target)\n",
    "    \n",
    "    # rolling mean\n",
    "    if rolling_mean_target:\n",
    "        df[rolling_mean_target] =  window_mean(df, window_size=period, column_names=rolling_mean_target)\n",
    "    \n",
    "    df = dp.dropNaNrows(df)\n",
    "    \n",
    "    # Sample the data at period intervals\n",
    "    df = dp.sample_timeseries_df(df, period=period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oat</th>\n",
       "      <th>sat</th>\n",
       "      <th>sat_stpt</th>\n",
       "      <th>orh</th>\n",
       "      <th>hwe</th>\n",
       "      <th>cwe</th>\n",
       "      <th>ghi</th>\n",
       "      <th>avg_stpt</th>\n",
       "      <th>flow</th>\n",
       "      <th>hw_rt</th>\n",
       "      <th>hw_sf</th>\n",
       "      <th>hw_st</th>\n",
       "      <th>hw_s_stp</th>\n",
       "      <th>hx_vlv1</th>\n",
       "      <th>wbt</th>\n",
       "      <th>sat-oat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "      <td>21971.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>63.626614</td>\n",
       "      <td>70.279652</td>\n",
       "      <td>71.527172</td>\n",
       "      <td>60.052722</td>\n",
       "      <td>20.669878</td>\n",
       "      <td>22.974858</td>\n",
       "      <td>172.949942</td>\n",
       "      <td>71.543912</td>\n",
       "      <td>37.365195</td>\n",
       "      <td>91.813681</td>\n",
       "      <td>8.736043</td>\n",
       "      <td>101.209883</td>\n",
       "      <td>101.228952</td>\n",
       "      <td>25.113443</td>\n",
       "      <td>55.037905</td>\n",
       "      <td>6.652519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.450034</td>\n",
       "      <td>5.522785</td>\n",
       "      <td>2.912957</td>\n",
       "      <td>17.310180</td>\n",
       "      <td>24.971515</td>\n",
       "      <td>17.184686</td>\n",
       "      <td>256.256255</td>\n",
       "      <td>1.049008</td>\n",
       "      <td>30.230293</td>\n",
       "      <td>13.201565</td>\n",
       "      <td>9.362448</td>\n",
       "      <td>14.165990</td>\n",
       "      <td>14.217190</td>\n",
       "      <td>19.756852</td>\n",
       "      <td>14.707197</td>\n",
       "      <td>17.865938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.821972</td>\n",
       "      <td>51.414037</td>\n",
       "      <td>58.605072</td>\n",
       "      <td>17.309428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.155953</td>\n",
       "      <td>-52.457182</td>\n",
       "      <td>66.099812</td>\n",
       "      <td>-1.813372</td>\n",
       "      <td>64.903137</td>\n",
       "      <td>-2.463692</td>\n",
       "      <td>80.075723</td>\n",
       "      <td>87.778581</td>\n",
       "      <td>-6.725714</td>\n",
       "      <td>14.846887</td>\n",
       "      <td>-25.245728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>49.121291</td>\n",
       "      <td>67.350621</td>\n",
       "      <td>72.986150</td>\n",
       "      <td>46.642814</td>\n",
       "      <td>0.353918</td>\n",
       "      <td>6.788249</td>\n",
       "      <td>-0.008301</td>\n",
       "      <td>70.895253</td>\n",
       "      <td>9.579592</td>\n",
       "      <td>82.487393</td>\n",
       "      <td>0.153497</td>\n",
       "      <td>90.170177</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>8.603475</td>\n",
       "      <td>42.724135</td>\n",
       "      <td>-6.342551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>65.803442</td>\n",
       "      <td>71.940837</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>59.389471</td>\n",
       "      <td>12.052000</td>\n",
       "      <td>21.382697</td>\n",
       "      <td>14.551819</td>\n",
       "      <td>71.342909</td>\n",
       "      <td>32.586087</td>\n",
       "      <td>87.652988</td>\n",
       "      <td>5.768496</td>\n",
       "      <td>93.361023</td>\n",
       "      <td>91.824746</td>\n",
       "      <td>24.093105</td>\n",
       "      <td>57.820886</td>\n",
       "      <td>2.493411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>77.806630</td>\n",
       "      <td>73.671614</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>74.189350</td>\n",
       "      <td>29.194800</td>\n",
       "      <td>38.433394</td>\n",
       "      <td>284.159439</td>\n",
       "      <td>72.485714</td>\n",
       "      <td>63.573852</td>\n",
       "      <td>98.307198</td>\n",
       "      <td>13.121573</td>\n",
       "      <td>112.266250</td>\n",
       "      <td>112.209060</td>\n",
       "      <td>42.577149</td>\n",
       "      <td>68.115798</td>\n",
       "      <td>23.150384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>97.319737</td>\n",
       "      <td>90.015817</td>\n",
       "      <td>74.044062</td>\n",
       "      <td>95.007589</td>\n",
       "      <td>160.411511</td>\n",
       "      <td>66.524093</td>\n",
       "      <td>973.479051</td>\n",
       "      <td>73.557921</td>\n",
       "      <td>125.398265</td>\n",
       "      <td>135.552460</td>\n",
       "      <td>32.572670</td>\n",
       "      <td>147.479417</td>\n",
       "      <td>148.331329</td>\n",
       "      <td>73.261725</td>\n",
       "      <td>85.891934</td>\n",
       "      <td>55.312001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                oat           sat      sat_stpt           orh           hwe  \\\n",
       "count  21971.000000  21971.000000  21971.000000  21971.000000  21971.000000   \n",
       "mean      63.626614     70.279652     71.527172     60.052722     20.669878   \n",
       "std       17.450034      5.522785      2.912957     17.310180     24.971515   \n",
       "min       17.821972     51.414037     58.605072     17.309428      0.000000   \n",
       "25%       49.121291     67.350621     72.986150     46.642814      0.353918   \n",
       "50%       65.803442     71.940837     73.000000     59.389471     12.052000   \n",
       "75%       77.806630     73.671614     73.000000     74.189350     29.194800   \n",
       "max       97.319737     90.015817     74.044062     95.007589    160.411511   \n",
       "\n",
       "                cwe           ghi      avg_stpt          flow         hw_rt  \\\n",
       "count  21971.000000  21971.000000  21971.000000  21971.000000  21971.000000   \n",
       "mean      22.974858    172.949942     71.543912     37.365195     91.813681   \n",
       "std       17.184686    256.256255      1.049008     30.230293     13.201565   \n",
       "min       -3.155953    -52.457182     66.099812     -1.813372     64.903137   \n",
       "25%        6.788249     -0.008301     70.895253      9.579592     82.487393   \n",
       "50%       21.382697     14.551819     71.342909     32.586087     87.652988   \n",
       "75%       38.433394    284.159439     72.485714     63.573852     98.307198   \n",
       "max       66.524093    973.479051     73.557921    125.398265    135.552460   \n",
       "\n",
       "              hw_sf         hw_st      hw_s_stp       hx_vlv1           wbt  \\\n",
       "count  21971.000000  21971.000000  21971.000000  21971.000000  21971.000000   \n",
       "mean       8.736043    101.209883    101.228952     25.113443     55.037905   \n",
       "std        9.362448     14.165990     14.217190     19.756852     14.707197   \n",
       "min       -2.463692     80.075723     87.778581     -6.725714     14.846887   \n",
       "25%        0.153497     90.170177     90.000000      8.603475     42.724135   \n",
       "50%        5.768496     93.361023     91.824746     24.093105     57.820886   \n",
       "75%       13.121573    112.266250    112.209060     42.577149     68.115798   \n",
       "max       32.572670    147.479417    148.331329     73.261725     85.891934   \n",
       "\n",
       "            sat-oat  \n",
       "count  21971.000000  \n",
       "mean       6.652519  \n",
       "std       17.865938  \n",
       "min      -25.245728  \n",
       "25%       -6.342551  \n",
       "50%        2.493411  \n",
       "75%       23.150384  \n",
       "max       55.312001  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get mean of the entire scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# get the mean of the outputs for the entire data\n",
    "dfscaled = ((df-df.min())/(df.max()-df.min()))\n",
    "threshold_scaled = ((threshold-df.min()['hwe'])/(df.max()['hwe']-df.min()['hwe']))\n",
    "dfscaled = dfscaled[dfscaled['hwe']>=threshold_scaled]\n",
    "dfmean = dfscaled.mean() \n",
    "mean_output = list(dfmean[outputcols])\n",
    "mean_input = list(dfmean[inputcols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create temporal chunks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list of \"days\" day dataframes for training\n",
    "dflist = dp.df2dflist_alt(df[inputcols+outputcols],\n",
    "                      subsequence=True,\n",
    "                      period=period,\n",
    "                      days=days,\n",
    "                      hours=hours)\n",
    "print('Length of dflist: {}'.format(len(dflist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom way to create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "assert (input_timesteps == 1) & (\n",
    "    output_timesteps == 1), \"Input and Output timesteps must be 1 for this notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from pandas import concat\n",
    "import swifter\n",
    "\n",
    "\n",
    "def quickmerge(listdf):\n",
    "    return concat(listdf)\n",
    "\n",
    "\n",
    "def df2operating_regions(df, column_names, thresholds):\n",
    "    \"\"\"\n",
    "    Select from data frame the operating regions based on threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    org_shape = df.shape[0]\n",
    "    \n",
    "    # select cells to be retained\n",
    "    constraints = df.swifter.apply(\n",
    "        lambda row: all([(cell > thresholds) for cell in row[column_names]]),\n",
    "        axis=1)\n",
    "    # Drop values set to be rejected\n",
    "    df = df.drop(df.index[~constraints], axis = 0)\n",
    "    \n",
    "    print(\"Retaining {}% of the data\".format(100*df.shape[0]/org_shape))\n",
    "    \n",
    "    return df\n",
    "\n",
    "weeklist = []  # create list of training, testing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# select and merge data_weeks-1 worth of data\n",
    "datablock_train_pre = dflist[startweek:data_weeks-1]\n",
    "datablock_train = quickmerge(datablock_train_pre)\n",
    "#Select from data frame the operating regions\n",
    "datablock_train = df2operating_regions(datablock_train, outputcols, threshold)\n",
    "\"\"\"\n",
    "#divide into contiguous regions\n",
    "datablock_train = dp.subsequencing(datablock_train, period=period)\n",
    "#drop smaller blocks\n",
    "datablock_train = [item for item in datablock_train if item.shape[0] >= 0.05*modelconfig['train_batchsize']]\n",
    "#merge them together\n",
    "minibatch_train = quickmerge(datablock_train)\n",
    "\"\"\"\n",
    "minibatch_train = datablock_train\n",
    "\n",
    "# select weeks=1 worth of data\n",
    "datablock_test_pre = dflist[data_weeks-1]\n",
    "#Select from data frame the operating regions\n",
    "datablock_test = df2operating_regions(datablock_test_pre, outputcols, threshold)\n",
    "\"\"\"\n",
    "#divide into contiguous operating region\n",
    "datablock_test = dp.subsequencing(datablock_test_pre, period=period)\n",
    "#drop smaller blocks\n",
    "datablock_test = [item for item in datablock_test if item.shape[0] >= 0.05*modelconfig['test_batchsize']]\n",
    "#merge them together\n",
    "minibatch_test = quickmerge(datablock_test)\n",
    "\"\"\"\n",
    "minibatch_test = datablock_test\n",
    "\n",
    "\n",
    "# splitvalue\n",
    "splitvalue = minibatch_test.shape[0]\n",
    "#merge test and train together\n",
    "data_block = quickmerge([minibatch_train, minibatch_test])\n",
    "\n",
    "# create numpy arrays\n",
    "X_train, X_test, y_train, y_test, X_scaler, y_scaler = dp.df2arrays(\n",
    "        data_block,\n",
    "        predictorcols=inputcols,\n",
    "        outputcols=outputcols,\n",
    "        scaling=scaling,\n",
    "        feature_range=feature_range,\n",
    "        reshaping=reshaping,\n",
    "        lag=create_lag,\n",
    "        split=splitvalue,\n",
    "    input_timesteps=input_timesteps,\n",
    "    output_timesteps = output_timesteps\n",
    "    )\n",
    "\n",
    "# select test ids for later plots\n",
    "test_idx = minibatch_test.index\n",
    "\n",
    "# year and week\n",
    "yearno = minibatch_test.index[int(splitvalue/2)].year\n",
    "weekno = minibatch_test.index[int(splitvalue/2)].week\n",
    "\n",
    "# append them\n",
    "weeklist.append({\n",
    "        'Id':'Year-{}-Week-{}'.format(str(yearno), \n",
    "                                      str(weekno)),\n",
    "        'X_train':X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_scaler':y_scaler,\n",
    "        'X_scaler':X_scaler,\n",
    "        'test_idx':test_idx,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weekdata in dflist[data_weeks:end_week]:\n",
    "    \n",
    "    # select and merge data_weeks-1 worth of data\n",
    "    datablock_train_pre = datablock_train_pre[1:]+[datablock_test_pre]\n",
    "    datablock_train = quickmerge(datablock_train_pre)\n",
    "    #Select from data frame the operating regions\n",
    "    datablock_train = df2operating_regions(datablock_train, outputcols, threshold)\n",
    "    minibatch_train = datablock_train\n",
    "    \n",
    "    # select weeks=1 worth of data\n",
    "    datablock_test_pre = weekdata\n",
    "    #Select from data frame the operating regions\n",
    "    datablock_test = df2operating_regions(datablock_test_pre, outputcols, threshold)\n",
    "    minibatch_test = datablock_test\n",
    "    \n",
    "    # splitvalue\n",
    "    splitvalue = minibatch_test.shape[0]\n",
    "    #merge test and train together\n",
    "    data_block = quickmerge([minibatch_train, minibatch_test])\n",
    "\n",
    "    # and add new week data from weekdata\n",
    "    X_train, X_test, y_train, y_test, X_scaler, y_scaler = dp.df2arrays(\n",
    "        data_block,\n",
    "        predictorcols=inputcols,\n",
    "        outputcols=outputcols,\n",
    "        scaling=scaling,\n",
    "        feature_range=feature_range,\n",
    "        reshaping=reshaping,\n",
    "        lag=create_lag,\n",
    "        split=splitvalue,\n",
    "    input_timesteps=input_timesteps,\n",
    "    output_timesteps = output_timesteps\n",
    "    )\n",
    "\n",
    "    # select test ids for later plots\n",
    "    test_idx = minibatch_test.index\n",
    "    \n",
    "    # year and week no\n",
    "    weekno += 1\n",
    "    weekno = weekno if weekno%53 != 0 else 1\n",
    "    yearno = yearno if weekno!= 1 else yearno+1\n",
    "\n",
    "    weeklist.append({\n",
    "        'Id':'Year-{}-Week-{}'.format(str(yearno), \n",
    "                                      str(weekno)),\n",
    "        'X_train':X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_scaler':y_scaler,\n",
    "        'X_scaler':X_scaler,\n",
    "        'test_idx':test_idx,\n",
    "    })\n",
    "\n",
    "print('Length of weeklist: {}'.format(len(weeklist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print size and shape of data to feed to the model for sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for week in weeklist:\n",
    "    for key,value in week.items():\n",
    "        if (key != 'y_scaler') & (key != 'X_scaler') :\n",
    "            print(\"name: {}, shape: {}\".format(key, value.shape if not isinstance(value,str) else value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weekly train test data to modelconfig dictionary for ease of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "modelconfig['weeklist'] = weeklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clear the Tensorflow graph from previous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# weekdata = modelconfig['weeklist'][1]\n",
    "# X_train = weekdata['X_train']\n",
    "# y_train = weekdata['y_train']\n",
    "# X_test = weekdata['X_test']\n",
    "# y_test = weekdata['y_test']\n",
    "# y_scaler = weekdata['y_scaler']\n",
    "\n",
    "# #Instantiate learner model\n",
    "# nn_model = mp.simple_LSTM_model(path,\n",
    "#                       inputdim=X_train.shape[-1],\n",
    "#                       outputdim=y_train.shape[-1],\n",
    "#                       period=period)\n",
    "\n",
    "# # Desing model architecture\n",
    "# nn_model.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * (2*modelconfig['lstm_no_layers']),\n",
    "#                    densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "#                    dropoutlist=[[], []],\n",
    "#                    batchnormalizelist=[[], []])\n",
    "\n",
    "# # compile model\n",
    "# nn_model.model_compile()\n",
    "# print(K.get_session())\n",
    "\n",
    "# # nn_model.model.summary()0x7f8d2b89f048  0x7f8ba4767550>\n",
    "\n",
    "# #Instantiate learner model\n",
    "# nn_model2 = mp.simple_LSTM_model(path,\n",
    "#                       inputdim=X_train.shape[-1],\n",
    "#                       outputdim=y_train.shape[-1],\n",
    "#                       period=period)\n",
    "\n",
    "# # Desing model architecture\n",
    "# nn_model2.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * (2*modelconfig['lstm_no_layers']),\n",
    "#                    densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "#                    dropoutlist=[[], []],\n",
    "#                    batchnormalizelist=[[], []])\n",
    "\n",
    "# # compile model\n",
    "# nn_model.model_compile()\n",
    "\n",
    "# print(K.get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weekno, BatchData in enumerate(tqdm(modelconfig['weeklist'])):\n",
    "    \n",
    "    X_train = BatchData['X_train']\n",
    "    y_train = BatchData['y_train']\n",
    "    y_scaler = BatchData['y_scaler']\n",
    "    X_scaler = BatchData['X_scaler']\n",
    "    X_test = BatchData['X_test']\n",
    "    y_test = BatchData['y_test']\n",
    "\n",
    "    \"\"\"Begin weekly training\"\"\"   \n",
    "    K.clear_session()\n",
    "    \n",
    "    try:\n",
    "        del nn_model\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    #Instantiate learner model\n",
    "    nn_model = mp.regression_nn(path,\n",
    "                                  inputdim=X_train.shape[-1],\n",
    "                                  outputdim=y_train.shape[-1],\n",
    "                                  input_timesteps=input_timesteps,\n",
    "                                  output_timesteps = output_timesteps,\n",
    "                                  period=period,\n",
    "                                  stateful = modelconfig['train_stateful'],\n",
    "                                  batch_size=modelconfig['train_batchsize'])\n",
    "\n",
    "    # Desing model architecture\n",
    "    nn_model.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * modelconfig['lstm_no_layers'],\n",
    "                            densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "                            dropoutlist=[[], []],\n",
    "                            batchnormalizelist=[[], []])\n",
    "\n",
    "    # load the trained model weights if we want to: here some layer weights may be reinitialized; see below\n",
    "    if model_saved & retain_prev_model:\n",
    "        nn_model.model.load_weights('IntermediateModel.h5')\n",
    "\n",
    "\n",
    "    # compile model\n",
    "    nn_model.model_compile()   \n",
    "\n",
    "    # train the model for adaptive model and fixed after first round for fixed control\n",
    "    if adaptive_control | (weekno==0):\n",
    "        history = nn_model.train_model(X_train,\n",
    "                                       y_train,\n",
    "                                       X_test,\n",
    "                                       y_test,\n",
    "                                       epochs=modelconfig['train_epochs'],\n",
    "                                       initial_epoch=initial_epoch)\n",
    "        try:\n",
    "            initial_epoch += len(history.history['loss'])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        # save the model only if trained at least once- needed for prediction model\n",
    "        nn_model.model.save('IntermediateModel.h5')\n",
    "        model_saved = True     \n",
    "    \"\"\"End Weekly Training\"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"Begin Last Week Prediction\"\"\"\n",
    "    try:\n",
    "        del nn_model_pred\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    # Separate predictor for predicting online: only difference is test batch size\n",
    "    nn_model_pred = mp.regression_nn(path,\n",
    "                                       inputdim=X_test.shape[-1],\n",
    "                                       outputdim=y_test.shape[-1],\n",
    "                                       input_timesteps=input_timesteps,\n",
    "                                       output_timesteps = output_timesteps,\n",
    "                                       period=period,\n",
    "                                       stateful = modelconfig['test_stateful'],\n",
    "                                       batch_size=modelconfig['test_batchsize'])\n",
    "\n",
    "    # Desing model architecture\n",
    "    nn_model_pred.design_network(lstmhiddenlayers=[modelconfig['lstm_hidden_units']] * modelconfig['lstm_no_layers'],\n",
    "                            densehiddenlayers=[modelconfig['dense_hidden_units']] * modelconfig['dense_no_layers'],\n",
    "                            dropoutlist=[[], []],\n",
    "                            batchnormalizelist=[[], []])\n",
    "\n",
    "\n",
    "    # load the trained model weights\n",
    "    nn_model_pred.model.load_weights('IntermediateModel.h5')\n",
    "    # compile model\n",
    "    nn_model_pred.model_compile()\n",
    "\n",
    "    # evaluate the model for metrics at this stage\n",
    "    # train and test plots as well as logged errors inside the text file\n",
    "    preds_test = nn_model_pred.evaluate_model( X_test,\n",
    "                                               y_test,\n",
    "                                               y_scaler,\n",
    "                                               save_plot_loc=path+'normalplots/',\n",
    "                                               scaling=True,\n",
    "                                               saveplot=True,\n",
    "                                               Idx=BatchData['Id'],\n",
    "                                               outputdim_names=[addl['names_abreviation'][outputcols[0]]],\n",
    "                                               output_mean = mean_output)\n",
    "\n",
    "    # do a detailed plot instead\n",
    "    pu.regression_plot(period * 5,\n",
    "                    xs = date2num(list(BatchData['test_idx'])),\n",
    "                    outputdim=len(outputcols),\n",
    "                    output_timesteps=output_timesteps,\n",
    "                    input_timesteps=input_timesteps,\n",
    "                    pred=preds_test,\n",
    "                    target=y_test,\n",
    "                    X_var=X_test,\n",
    "                    x_loc=x_loc,\n",
    "                    x_lab=x_lab,\n",
    "                    saveloc=path + 'detailedplots/',\n",
    "                    scaling=True,\n",
    "                    Xscaler=X_scaler,\n",
    "                    yscaler=y_scaler,\n",
    "                    outputdim_names=[addl['names_abreviation'][outputcols[0]]],\n",
    "                    typeofplot='test',\n",
    "                    Idx=BatchData['Id'])\n",
    "    \"\"\"End Last Week Prediction\"\"\"\n",
    "\n",
    "    \"\"\"Only execute when we are freezing LSTM and just training on Dense\"\"\"\n",
    "    if adaptive_control:\n",
    "        \n",
    "        # freeze all but dense layers at the top and compile with new weights\n",
    "        if freeze_model:\n",
    "            for layer in nn_model.model.layers[:modelconfig['retrain_from_layers']]:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # for relearning, reinitialize top few layers\n",
    "        if reinitialize:\n",
    "            for layer in nn_model.model.layers[modelconfig['retrain_from_layers']:]:\n",
    "                layer.kernel.initializer.run(session=K.get_session())\n",
    "                layer.bias.initializer.run(session=K.get_session())\n",
    "\n",
    "        # recompile model\n",
    "        if freeze_model | reinitialize:\n",
    "            nn_model.model_compile()\n",
    "            \n",
    "            # save the model- needed for Keras limitations: Tensorboard crashes if we relearn on original model with\n",
    "            # reinitialized weights; solution: we create new model and load these weights\n",
    "            nn_model.model.save('IntermediateModel.h5')\n",
    "            model_saved = True\n",
    "            \n",
    "            freeze_model = False\n",
    "            reinitialize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the CVRMSE error on chunks of temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "metricname = 'cvrmse'\n",
    "# Open a file\n",
    "fo = open(path + \"{}min Results_File.txt\".format(5 * period), \"r\")\n",
    "print(\"Name of the file: \", fo.name)\n",
    "lines = fo.readlines()\n",
    "\n",
    "parse_format = 'Year-{}-Week-{}-Time Step {}: {} RMSE={} |{} CVRMSE={} |{} MAE={}'\n",
    "stats = {\n",
    "    'Train': {\n",
    "        'rmse': [],\n",
    "        'cvrmse': [],\n",
    "        'mae': []\n",
    "    },\n",
    "    'Test': {\n",
    "        'rmse': [],\n",
    "        'cvrmse': [],\n",
    "        'mae': []\n",
    "    }\n",
    "}\n",
    "xticklist = []\n",
    "counter = 0\n",
    "for line in lines:\n",
    "    p = parse.parse(parse_format, line)\n",
    "    stats[p[3]]['rmse'].append(float(p[4]))\n",
    "    stats[p[5]]['cvrmse'].append(float(p[6]))\n",
    "    stats[p[7]]['mae'].append(float(p[8]))\n",
    "    if counter % 1 == 0:\n",
    "        xticklist.append('Year-{}-Week-{}'.format(p[0], p[1]))\n",
    "    counter += 1\n",
    "fo.close()\n",
    "metric_list = stats['Test'][metricname]\n",
    "\n",
    "# max_metric = max(cvrmse_list)\n",
    "# metric_list = [i if i <= 30 else (10*(i-30)/(max_cvrmse-30))+30 for i in cvrmse_list]\n",
    "\n",
    "cvrmse = sum(metric_list) / len(metric_list)\n",
    "# from dataprocess import plotutils as pu\n",
    "plot_args = dict(\n",
    "    bars=metric_list,\n",
    "    color='goldenrod',\n",
    "    bar_label=metricname,\n",
    "    saveloc=path,\n",
    "    smoothcurve=True,\n",
    "    bar_annotate=True,\n",
    "    saveplot=True,\n",
    "    xlabel='Week of year',\n",
    "    ylabel='cvrmse error in percentage',\n",
    "    title=\n",
    "    'Weekly CVRMSE Error for Hot Water Energy Model at {5:}min(s) intervals \\n [{0:} \\\n",
    "    layers of {1:}-unit lstm, {2:} layers of {3:}-unit dense] \\n Average CVRMSE Error {4:.2f}%'\n",
    "    .format(modelconfig['lstm_no_layers'], modelconfig['lstm_hidden_units'],\n",
    "            modelconfig['dense_no_layers'], modelconfig['dense_hidden_units'],\n",
    "            cvrmse, 5 * period),\n",
    "    xticklist=xticklist,\n",
    "    plotwidth=17,\n",
    "    plotheight=6,\n",
    "    fontsize=16,\n",
    "    savetitle=metricname+' plot.png')\n",
    "pu.regression_bar_plot(**plot_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the pdfs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# path where files are stored\n",
    "pdfs_loc = path + 'detailedplots/'\n",
    "# list all the files\n",
    "flist = sorted(glob.glob(os.path.join(pdfs_loc, '*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    " \n",
    "# Call the PdfFileMerger\n",
    "mergedObject = PdfFileMerger()\n",
    " \n",
    "# I had 116 files in the folder that had to be merged into a single document\n",
    "# Loop through all of them and append their pages\n",
    "for filename in flist:\n",
    "    mergedObject.append(PdfFileReader(filename, 'rb'))\n",
    "    \n",
    "#  Write all the files into a file which is named as shown below\n",
    "mergedObject.write(path+'DetailedPredvsTarget.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "### Code cemetery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(nn_model.model, to_file='model.png',show_shapes=True,)\n",
    "\n",
    "\n",
    "# # Check the trainable status of the individual layers\n",
    "# for layer in nn_model.model.layers:\n",
    "#     print(layer, layer.trainable, layer.name)\n",
    "\n",
    "\n",
    "#model.model.summary()\n",
    "\n",
    "\n",
    "#nn_model.model.layers[-1].input_shape\n",
    "#model.model.layers[-1].kernel.initializer.run(session=K.get_session())\n",
    "#model.model.layers[-1].bias.initializer.run(session=K.get_session())\n",
    "#nn_model.model.layers[-1].output_shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('sbvenv1': venv)",
   "language": "python",
   "name": "python36964bitsbvenv1venv1a534851ebbc4d609aad5dcf7b359ab5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
